# These functions were taken from the LibreFlux Hugging Face and modified as needed

import torch
from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5TokenizerFast
from typing import List, Optional, Union

# Helper function refactored from the pipeline's _get_t5_prompt_embeds
def _get_t5_prompt_embeds_standalone(
    prompt: Union[str, List[str]],
    tokenizer_2: T5TokenizerFast,
    text_encoder_2: T5EncoderModel,
    max_sequence_length: int = 512,
    device: Optional[torch.device] = None,
):
    prompt = [prompt] if isinstance(prompt, str) else [prompt] if not isinstance(prompt, list) else prompt
    batch_size = len(prompt)

    text_inputs = tokenizer_2(
        prompt,
        padding="max_length",
        max_length=max_sequence_length,
        truncation=True,
        return_tensors="pt",
    )

    # This mask is generated by the tokenizer to ignore padding tokens
    prompt_attention_mask = text_inputs.attention_mask.to(device)
    text_input_ids = text_inputs.input_ids.to(device)

    prompt_embeds = text_encoder_2(text_input_ids, output_hidden_states=False)[0]

    return prompt_embeds.to(dtype=text_encoder_2.dtype), prompt_attention_mask

# Helper function refactored from the pipeline's _get_clip_prompt_embeds
def _get_clip_prompt_embeds_standalone(
    prompt: Union[str, List[str]],
    tokenizer: CLIPTokenizer,
    text_encoder: CLIPTextModel,
    device: Optional[torch.device] = None,
):
    prompt = [prompt] if isinstance(prompt, str) else [prompt] if not isinstance(prompt, list) else prompt

    text_inputs = tokenizer(
        prompt,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    )
    text_input_ids = text_inputs.input_ids.to(device)

    prompt_embeds = text_encoder(text_input_ids, output_hidden_states=False)
    pooled_prompt_embeds = prompt_embeds.pooler_output

    return pooled_prompt_embeds.to(dtype=text_encoder.dtype)

# The main function, refactored to be standalone
def encode_prompt_standalone(
    prompt: Union[str, List[str]],
    tokenizer_one: CLIPTokenizer,
    text_encoder_one: CLIPTextModel,
    tokenizer_two: T5TokenizerFast,
    text_encoder_two: T5EncoderModel,
    max_sequence_length: int = 512,
    device: Optional[torch.device] = None,
):
    """
    Encodes a prompt using both CLIP and T5 text encoders.

    Returns:
        A tuple containing: (prompt_embeds, pooled_prompt_embeds, text_ids, prompt_attention_mask)
    """

    # 1. Get pooled embeddings from CLIP
    pooled_prompt_embeds = _get_clip_prompt_embeds_standalone(
        prompt=prompt,
        tokenizer=tokenizer_one,
        text_encoder=text_encoder_one,
        device=device,
    )

    # 2. Get sequence embeddings and attention mask from T5
    prompt_embeds, prompt_attention_mask = _get_t5_prompt_embeds_standalone(
        prompt=prompt,
        tokenizer_2=tokenizer_two,
        text_encoder_2=text_encoder_two,
        max_sequence_length=max_sequence_length,
        device=device,
    )

    # 3. Create dummy text_ids (as the original pipeline does)
    if isinstance(prompt, str):
        batch_size = 1
    else:
        batch_size = len(prompt)

    text_ids = torch.zeros(
        batch_size,
        prompt_embeds.shape[1],
        3,
        device=device,
        dtype=prompt_embeds.dtype
    )

    return prompt_embeds, pooled_prompt_embeds, text_ids, prompt_attention_mask
